# sif-ai-form-service

Python microservice for the SIF AI form flow. Runs the DSPy planner and returns form steps as JSON.

## Endpoints

- `GET /health`
- `POST /v1/api/form`
- `GET /v1/api/form/capabilities` (JSON; contract schema + version)
- `POST /v1/api/image` (image prompt + image generation)

## API contract workflow (OpenAPI)

This repo commits its OpenAPI spec as a machine-enforced contract for frontend/backed integration.

- Update: `make export-openapi-contract`
- Verify (CI): `make verify-openapi-contract`

## Shared UI contract (vendored)

The canonical "UIStep contract" lives under `shared/ai-form-ui-contract/`:
- `shared/ai-form-ui-contract/schema/schema_version.txt`
- `shared/ai-form-ui-contract/schema/ui_step.schema.json`
- `shared/ai-form-ui-contract/schema/ui_step.types.ts`
- `shared/ai-form-ui-contract/demos/next_steps_examples.jsonl`

This folder is committed so the service can return `schemaVersion` + `uiStepSchema` via `GET /v1/api/form/capabilities`.

## Local dev

**Quick start:** Copy `.env.example` to `.env` and fill in your values. The app auto-loads `.env` and `.env.local` (no `source .env` needed).

**Required env vars:**

**DSPy (for LLM calls):**
- `DSPY_PROVIDER=groq` (or `openai`)
- `GROQ_API_KEY=...` (or `OPENAI_API_KEY=...`)
- `DSPY_MODEL_LOCK=llama-3.3-70b-versatile` (optional)

**Optional per-module overrides (Planner vs Renderer):**
- Planner:
  - `DSPY_PLANNER_PROVIDER`, `DSPY_PLANNER_MODEL_LOCK`, `DSPY_PLANNER_MODEL`
  - `DSPY_PLANNER_TEMPERATURE`, `DSPY_PLANNER_TIMEOUT_SEC`, `DSPY_PLANNER_MAX_TOKENS`
- Renderer:
  - `DSPY_RENDERER_PROVIDER`, `DSPY_RENDERER_MODEL_LOCK`, `DSPY_RENDERER_MODEL`
  - `DSPY_RENDERER_TEMPERATURE`, `DSPY_RENDERER_TIMEOUT_SEC`, `DSPY_RENDERER_MAX_TOKENS`

**Optional:**
- `DSPY_NEXT_STEPS_DEMO_PACK=/absolute/or/repo/relative/path.jsonl` (optional override)
  - Default demo pack is `shared/ai-form-ui-contract/demos/next_steps_examples.jsonl` if present
  - Schema version is read from `shared/ai-form-ui-contract/schema/schema_version.txt`
- Image generation:
  - `IMAGE_PROVIDER=mock` (default; returns SVG data URLs)
  - `DSPY_IMAGE_PROMPT_MAX_TOKENS=900` (prompt-builder token cap)
- Pipeline toggles:
  - `AI_FORM_RENDER_CACHE=true` + `AI_FORM_RENDER_CACHE_TTL_SEC=600` (cache validated `miniSteps[]` in-memory)
  - `AI_FORM_LOG_LATENCY=true` (emit a structured `step3_latency` log line per request)

Install + run:

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
# Either entrypoint works:
uvicorn api.main:app --reload --port 8008
# uvicorn api.main:app --reload --port 8008 (legacy entry point)
```

Test health:

```bash
curl -s http://localhost:8008/health | jq
```

Test form generation (JSON):

```bash
curl -X POST http://localhost:8008/api/form \
  -H 'content-type: application/json' \
  -d '{"mode":"next_steps","batchId":"ContextCore","platformGoal":"test","businessContext":"test","industry":"General","service":"","requiredUploads":[],"personalizationSummary":"","stepDataSoFar":{},"alreadyAskedKeys":[],"batchState":{},"allowedMiniTypes":["multiple_choice"],"maxSteps":3}'
```

Test form generation using the widget request shape (JSON):

```bash
curl -X POST http://localhost:8008/v1/api/form \
  -H 'content-type: application/json' \
  -d '{"session":{"sessionId":"sess_test","instanceId":"inst_test"},"currentBatch":{"batchId":"batch-1","batchNumber":1,"maxSteps":5},"state":{"answers":{"step-service-primary":"abc"},"askedStepIds":["step-service-primary"]},"request":{"noCache":true,"schemaVersion":"dev"}}'
```

Test image generation (JSON):

```bash
curl -X POST http://localhost:8008/api/image \
  -H 'content-type: application/json' \
  -d '{"instanceId":"uuid-here","useCase":"scene","numOutputs":2,"outputFormat":"url","stepDataSoFar":{"step-space-type":"kitchen","step-budget":"5000"},"config":{"platformGoal":"AI pre-design intake","businessContext":"We generate AI images for early design concepts","industry":"Interior Design","service":"Kitchen Remodel","personalizationSummary":"Bright, warm, natural materials"}}'
```

## Deploy to Vercel

This repo is set up as a Vercel **Python Serverless Function** with a catch-all route to `api/index.py`
(see `vercel.json`).

### Deployment Protection (401 Authentication Required)

If `GET /health` returns **401 Authentication Required**, your deployment is behind **Vercel Deployment Protection**
(sometimes shown as “Vercel Authentication”).

You have two options:

- **Make it public (recommended)**: Vercel Dashboard → Project → Settings → Deployment Protection → set **Production** to **Disabled/Public**.
- **Keep protection ON (automation bypass)**: enable **Protection Bypass for Automation** in the same settings page and use the
  generated bypass secret in your server-to-server requests:
  - Send header **`x-vercel-protection-bypass: <BYPASS_SECRET>`**
  - Vercel also exposes this secret to the deployment as **`VERCEL_AUTOMATION_BYPASS_SECRET`**

In Vercel Project Settings, set required env vars (same as local dev):
- `DSPY_PROVIDER`
- `DSPY_MODEL_LOCK` (optional)
- `GROQ_API_KEY` (or `OPENAI_API_KEY` if using `DSPY_PROVIDER=openai`)

Deploy:
- **Git**: import the repo in Vercel and deploy from the dashboard
- **CLI**: from the repo root, run `vercel` (and `vercel --prod` when ready)

After deploy, verify:

```bash
curl -s https://YOUR_VERCEL_DOMAIN/health | jq
```
